---
title: "Merging Gentzkow Replication Data with Census Places"
author: "nwfried"
date: "2024-03-18"
output: pdf_document
---
# Data Wrangling
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Initialising libraries.
```{r}
library(tidyverse)
library(fedmatch)
```
Loading in data.
```{r}
cities <- read_tsv("30261-0006-Data.tsv")
census_places <- read_csv("places2msa1970.csv")
```

First, we need to deal with the fact that Gentzkow's cities dataframe contains city names in all caps and the Census dataframe doesn't. This won't work with fuzzy match, so let's format both of these in all lowercase.
```{r}
cities$cityname_constant <- tolower(cities$cityname_constant)
census_places$NAME <- tolower(census_places$NAME)
census_places$PLACE <- tolower(census_places$PLACE)
```
Next, we can fuzzy match based on the "NAME" column. 
```{r}
fuzzy_merge <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid")

name_result <- fuzzy_merge$matches
```
Let's look at this result.
```{r}
head(name_result)
```
The problem here is that many place names are often reused, so the fuzzy match will match cities with the same name in different states e.g. Springfield, Illinois is matched will all other Springfields. We can use the county numbers of each city (found in the cnty90 and fips_2 columns, respectively) to make sure that the cities matched by the fuzzy match are actually the same. (we could also check this with state names, but I thought more specificty would be better at avoiding false positives).

First, we have to account for the fact that the four-digit county numbers in the Census dataframe are prefixed with a "0," while the ones in the city databse are not. Let's add this 0 to the city database.
```{r}
cnty_number <- cities$cnty90
fixed_cnty_number <- sprintf("%05d", cnty_number) #fix to 5 character width
cities <- cbind(fixed_cnty_number, cities) #add to dataset
```
Now we can do another fuzzy match and then filter out only the entries with matching county numbers.
```{r}
merge2 <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid")
result2 <- merge2$matches
unmatched_census <- merge2$data1_nomatch
unmatched_cities <- merge2$data2_nomatch
```

```{r}
filtered_result2 <- filter(result2, fixed_cnty_number == fips_2)
head(filtered_result2)
```
This gives us a list of 289 matched cities and census places. This is around a tenth of size of the original Gentzkow dataset and a fifth of the size of the initial fuzzy match merge. I'm wondering if there's some issue with the county filtering in that it's a bit too strict, given that ~1000 matches were dropped because the county numbers didn't match.

We can try filtering by state to see if this relaxes restrictions a bit. Again, we must set both datasets's state column to lowercase to deal with case sensitivity.
```{r}
cities$state <- tolower(cities$state)
census_places$STATE <- tolower(census_places$STATE)

state_merge <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid") # merge again with changes to datasets
state_result <- state_merge$matches

filtered_state_result <- filter(state_result, state == STATE) # create dataset with matching states
head(filtered_state_result)

state_merge_residue <- filter(state_result, state != STATE) # preserve data without matching states for later analysis
```
This returns around 540 matched cities and census places, filtering by state name.

# Analysis

## False matches
We can make it a little easier to look at this dataset by extracting only the most relevant columns.
```{r}
extracted_state_result <- filtered_state_result[,c("STATE", "PLACE", "namemsa", "NAME", "cityname_constant", "state")]
```

```{r}
filter(extracted_state_result, cityname_constant != NAME)
```
Filtering this dataset for place names that do not match yields only these three results. This should mean that the only entries with different city names are these three, of which the Los Altos/Los Gatos match seems to be the only false association.

Thus the only possible remaining false matches will be cities with the same name in the same state, which is hopefully only a handful. (I tried doing some research on this but I could only find various Reddit/Quora threads; there wasn't a definitive answer on how many of these places exist.)

## Manual Matching
We can try to bring the major cities that have been dropped back into this merge manually.

Creating a dataframe to store our manual matches:
```{r}
manual_matches <- head(cities, 0)
manual_matches <- cbind(manual_matches, head(census_places, 0))
# colnames(manual_matches) <- c(paste0("cities_", colnames(cities)), paste0("census_places_", colnames(census_places)))
colnames(manual_matches) <- c(paste0(colnames(cities)), paste0(colnames(census_places)))
```

Let's first try matching Manhattan-New York manually:
```{r}
# Manhattan
match_nyc_city <- cities[cities$citypermid == 148, ]
match_nyc_census <- census_places[census_places$PLACE == "new york city", ]
manual_matches <- rbind(manual_matches, c(match_nyc_city, match_nyc_census))
```
This is kind of tedious, we can write a function to speed it up a bit:
```{r}
new_manual_match <- function(manual_matches, id_cities, id_census_places) {
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$PLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- rbind(manual_matches, new_match)
  return(manual_matches)
}
```
```{r}
new_manual_match_nhgis <- function(manual_matches, id_cities, id_census_places) {
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$NHGISPLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- rbind(manual_matches, new_match)
  return(manual_matches)
}
```

Let's test the function:
```{r}
# Test with Bronx-New York match
manual_matches <- new_manual_match(manual_matches, 1944, "new york city")
```

Now we can use a hashmap to store all of our manual matches and just pass that to the function. That way if we want to add a new match we can just add it to the hashmap.

```{r}
manual_matches_map <- list(
  "queens" = list(permid = 2042, place= "new york city"),
  "brooklyn" = list(permid = 394, place= "new york city"),
  "staten island" = list(permid = 1647, place = "new york city"),
  "south chicago heights" = list(permid = 766, place = "south chicago heights village"),
  "austin texas" = list(permid = 549, place = "austin city"),
  "columbus ohio" = list(permid = 386, place = "columbus city"),
  "indianapolis" = list(permid = 319, place = "indianapolis city (remainder)"), 
  "nashville" = list(permid = 47037, place = "nashville-davidson metropolitan government (balance)"),
  "las vegas" = list(permid = 1839, place = "las vegas city"),
  "mesa" = list(permid = 20, place = "mesa city"),
  "miami" = list(permid = 62, place = "miami city"), 
  "virginia beach, VA" = list(permid = 215, place = "virginia beach city"), 
  "lexington, KY" = list(permid = 575, place = "lexington-fayette"),
  "chandler, AZ" = list(permid = 2143, place = "chandler city"),
  "st. petersburg, FL" = list(permid = 64, place = "st. petersburg city"),
  "st. petersburg, FL pt II" = list(permid = 64, place = "st. petersburg beach city"),
  "chesapeake, VA" = list(permid = 215, place = "chesapeake city"),
  "scottsdale, AZ" = list(permid = 20, place = "scottsdale city"),
  "tempe, AZ" = list(permid = 20, place = "tempe city"),
  "boise, ID" = list(permid = 313, place = "boise city city"),
  "norfolk, VA" = list(permid = 215, place = "norfolk city"),
  "portsmouth, VA" = list(permid = 215, place = "portsmouth city"),
  "fremont, CA" = list(permid = 31, place = "fremont city"),
  "fayetteville, NC"= list(permid = 1529, place = "fayetteville city"),
  "columbus, GA" = list(permid = 453, place = "columbus city (remainder)"),
  "olathe, KS" = list(permid = 98, place = "olathe city"),
  "overland park, KS" = list(permid = 98, place = "overland park city"),
  "grand rapids, MI" = list(permid = 485, place = "grand rapids city"),
  "newport news, VA" = list(permid = 214, place = "newport news city"),
  "mobile, AL" = list(permid = 15, place = "mobile city"),
  "prichard, AL" = list(permid = 15, place = "prichard city"),
  "chickasaw, AL" = list(permid = 15, place = "chickasaw city"),
  "oceanside, CA" = list(permid = 29, place = "oceanside city"),
  "carlsbad city, CA" = list(permid = 29, place = "carlsbad city"),
  "escondido city, CA" = list(permid = 29, place = "escondido city"),
  "vista city, CA" = list(permid = 29, place = "vista city")
)
```

Around halfway through the Wikipedia list of cities I noticed that there are a few Census places with duplicate names, which means we can't store them by referencing just the place name. We can get around this by instead storing and referencing the unique NHGISPLACE name for each entry. This is probably better practice anyway.
List for manual matches by NHGISPLACE instead of place name:
```{r}
manual_matches_map_nhgis <- list(
  "portland, OR" = list(permid = 533, nhgis = "G410590000"),
  "arlington, TX" = list(permid = 2023, nhgis = "G480040000"),
  "madison, WI" = list(permid = 559, nhgis = "G550480000"),
  "newark, CA" = list(permid = 31, nhgis = "G060509160"),
  "jacksonville, FL" = list(permid = 612, nhgis = "G120350000"),
  "rochester, NY" = list(permid = 519, nhgis = "G270548800"),
  "hampton, VA" = list(permid = 214, nhgis = "G510350000"),
  "ontario, CA" = list(permid = 38, nhgis = "G060538960"),
  "upland, CA" = list(permid = 38, nhgis = "G060813440"),
  "pomona, CA" = list(permid = 38, nhgis = "G060580720"),
  "springfield, MO" = list(permid = 712, nhgis = "G290700000"),
  "lanacster, CA" = list(permid = 34, nhgis = "G060401300"),
  "palmdale, CA" = list(permid = 34, nhgis = "G060551560"),
  "hollywood, FL" = list(permid = 2015, nhgis = "G120320000"),
  "springfield, MA" = list(permid = 334, nhgis = "G250670000"),
  "kansas city, KS" = list(permid = 901, nhgis = "G200360000"),
  "sunnyvale, CA" = list(permid = 47, nhgis = "G060770000"),
  "mountain view, CA" = list(permid = 47, nhgis = "G060496700"),
  "bridgeport, CT" = list(permid = 51, nhgis = "G090080000"),
  "pasadena, TX" = list(permid = 2024, nhgis = "G480560000"),
  "rockford, IL" = list(permid = 83, nhgis = "G170650000"),
  "loves park, IL" = list(permid = 83, nhgis = "G170450310"),
  "gainesville, FL" = list(permid = 756, nhgis = "G120251750"),
  "jackson, MS" = list(permid = 633, nhgis = "G280360000"),
  "columbia, SC" = list(permid = 544, nhgis = "G450160000"),
  "cedar rapids, IA" = list(permid = 92, nhgis = "G190120000"),
  "marion, IA" = list(permid = 92, nhgis = "G190494850"),
  "kent, WA" = list(permid = 218, nhgis = "G530354150"),
  "renton, WA" = list(permid = 218, nhgis = "G530577450"),
  "auburn, WA" = list(permid = 218, nhgis = "G530031800"),
  "fargo, ND" = list(permid = 154, nhgis = "G380257000"),
  "moorhead, ND" = list(permid = 154, nhgis = "G270438640"), #note: seems to be classified weirdly in Census dataset
  "carrollton, TX" = list(permid = 2186, nhgis = "G480130240"),
  "columbia, MO" = list(permid = 1315, nhgis = "G290156700"),
  "abilene, TX" = list(permid = 847, nhgis = "G480010000"),
  "college station, TX" = list(permid = 209, nhgis = "G480159760"),
  "bryan, TX" = list(permid = 209, nhgis = "G480109120"),
  "wilmington, NC" = list(permid = 527, nhgis = "G370744400"),
  "rochester, MN" = list(permid = 1028, nhgis = "G270548800"),
  "concord, CA" = list(permid = 1989, nhgis = "G060160000"),
  "independence, MO" = list(permid = 800, nhgis = "G290350000"),
  "fairfield, CA" = list(permid = 30, nhgis = "G060231820"),
  "suisun, CA" = list(permid = 30, nhgis = "G060756300"),
  "richmond, CA" = list(permid = 24, nhgis = "G060606200"),
  "berkeley, CA" = list(permid = 24, nhgis = "G060060000"),
  "clearwater, FL" = list(permid = 64, nhgis = "G120128750"),
  "manchester, NH" = list(permid = 501, nhgis = "G330451400"),
  "tuscaloosa, AL" = list(permid = 18, nhgis = "G010772560"),
  "northport, AL" = list(permid = 18, nhgis = "G010552000"),
  "ventura, CA" = list(permid = 303, nhgis = "G060650420"),
  "hillsboro, OR" = list(permid = 1600, nhgis = "G410341000"),
  "edinburg, TX" = list(permid = 1808, nhgis = "G480226600"),
  "west covina, CA" = list(permid = 45, nhgis = "G060842000"),
  "san gabriel, CA" = list(permid = 45, nhgis = "G060670420"),
  "south bend, IN" = list(permid = 91, nhgis = "G180710000"),
  "mishawka, IN" = list(permid = 91, nhgis = "G180499320"),
  "quincy, MA" = list(permid = 1014, nhgis = "G250557450"),
  "davenport, IA" = list(permid = 190, nhgis = "G190190000"),
  "bettendorf, IA" = list(permid = 190, nhgis = "G190063550")
)
```


Iterate over the first hashmap and add each entry to our manual_matches dataframe.
```{r}
for (i in manual_matches_map) {
  id_cities <- i$permid
  id_census_places <- i$place
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$PLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- bind_rows(manual_matches, new_match)
}
```
Iterate over the other hashmap and do the same thing.
```{r}
for (i in manual_matches_map_nhgis) {
  id_cities <- i$permid
  id_census_places <- i$nhgis
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$NHGISPLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- bind_rows(manual_matches, new_match)
}
```

### Takeaways:
Full information can be found on the [spreadsheet tracker](https://docs.google.com/spreadsheets/d/17T5rD9M4gP-UVLc_0cfu2rjyxg28vg85rVB60uc8Q9Q/edit?pli=1&gid=0#gid=0),but here are a few thoughts:
The Gentzkow data has an annoying habit of combining cities seemingly arbitrarily, especially in California for some reason, e.g. Richmond-Berkeley instead of Richmond and Berkeley. This is weird, because while these places are usually close to each other, they are distinct cities, and it causes the fuzzy match to fail. In the case that one of these cities appears in the Wikipedia dataset (so in this running example, if one of either Richmond or Berkely appears), I decided to match all associated Census places to this one Gentzkow entry.


