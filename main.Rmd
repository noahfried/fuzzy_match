---
title: "Merging Gentzkow Replication Data with Census Places"
author: "nwfried"
date: "2024-03-18"
output: pdf_document
---
# Data Wrangling
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Initialising libraries.
```{r}
library(tidyverse)
library(fedmatch)
```
Loading in data.
```{r}
cities <- read_tsv("30261-0006-Data.tsv")
census_places <- read_csv("places2msa1970.csv")
```

First, we need to deal with the fact that Gentzkow's cities dataframe contains city names in all caps and the Census dataframe doesn't. This won't work with fuzzy match, so let's format both of these in all lowercase.
```{r}
cities$cityname_constant <- tolower(cities$cityname_constant)
census_places$NAME <- tolower(census_places$NAME)
census_places$PLACE <- tolower(census_places$PLACE)
```
Next, we can fuzzy match based on the "NAME" column. 
```{r}
fuzzy_merge <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid")

name_result <- fuzzy_merge$matches
```
Let's look at this result.
```{r}
head(name_result)
```
The problem here is that many place names are often reused, so the fuzzy match will match cities with the same name in different states e.g. Springfield, Illinois is matched will all other Springfields. We can use the county numbers of each city (found in the cnty90 and fips_2 columns, respectively) to make sure that the cities matched by the fuzzy match are actually the same. (we could also check this with state names, but I thought more specificty would be better at avoiding false positives).

First, we have to account for the fact that the four-digit county numbers in the Census dataframe are prefixed with a "0," while the ones in the city databse are not. Let's add this 0 to the city database.
```{r}
cnty_number <- cities$cnty90
fixed_cnty_number <- sprintf("%05d", cnty_number) #fix to 5 character width
cities <- cbind(fixed_cnty_number, cities) #add to dataset
```
Now we can do another fuzzy match and then filter out only the entries with matching county numbers.
```{r}
merge2 <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid")
result2 <- merge2$matches
unmatched_census <- merge2$data1_nomatch
unmatched_cities <- merge2$data2_nomatch
```

```{r}
filtered_result2 <- filter(result2, fixed_cnty_number == fips_2)
head(filtered_result2)
```
This gives us a list of 289 matched cities and census places. This is around a tenth of size of the original Gentzkow dataset and a fifth of the size of the initial fuzzy match merge. I'm wondering if there's some issue with the county filtering in that it's a bit too strict, given that ~1000 matches were dropped because the county numbers didn't match.

We can try filtering by state to see if this relaxes restrictions a bit. Again, we must set both datasets's state column to lowercase to deal with case sensitivity.
```{r}
cities$state <- tolower(cities$state)
census_places$STATE <- tolower(census_places$STATE)

state_merge <- merge_plus(data1 = census_places,
           data2 = cities,
           by.x = "NAME",
           by.y = "cityname_constant", match_type = "fuzzy",
           unique_key_1 = "NHGISPLACE", unique_key_2 = "citypermid") # merge again with changes to datasets
state_result <- state_merge$matches

filtered_state_result <- filter(state_result, state == STATE) # create dataset with matching states
head(filtered_state_result)

state_merge_residue <- filter(state_result, state != STATE) # preserve data without matching states for later analysis
```
This returns around 540 matched cities and census places, filtering by state name.

# Analysis

## False matches
We can make it a little easier to look at this dataset by extracting only the most relevant columns.
```{r}
extracted_state_result <- filtered_state_result[,c("STATE", "PLACE", "namemsa", "NAME", "cityname_constant", "state")]
```

```{r}
filter(extracted_state_result, cityname_constant != NAME)
```
Filtering this dataset for place names that do not match yields only these three results. This should mean that the only entries with different city names are these three, of which the Los Altos/Los Gatos match seems to be the only false association.

Thus the only possible remaining false matches will be cities with the same name in the same state, which is hopefully only a handful. (I tried doing some research on this but I could only find various Reddit/Quora threads; there wasn't a definitive answer on how many of these places exist.)

## Unmatched cities
This is where it gets a bit more tricky.
First, we can look at the places that weren't matched at all in the fuzzy match: these seem to roughly fall into two categories. 

* Places like North Attleboro, MA -- there are a few cities near this in the unmatched Census data (same county) but no exact match. If we want to match these places, we could probably do this by looking at county number, but this might be dubious because they are ultimately different cities.
* Places like Dubois, PA -- there's nothing in this county in the Census data: presumably just not in the dataset.

There also seem to be some larger cities that are not getting matched because of differences in which they are labelled: for instance, "south chicago" exists in the unmatched_cities dataset and "south chicago heights" exists in the unmatched_census dataset. It might be prudent to go through some of the major cities to see if there are any other examples like this.

In general, major cities seem to be the biggest issue with this system of merging. I tried to look for New York to see how it was merged in this process, but I couldn't find the original cities entry for "manhattan" in either the unmatched cities dataset or the matched dataset. This led me to believe that it had been incorrectly matched (say, with Manhattan, Kansas) and subsequently filtered out, so I created a dataset (state_merge_residue) to keep track of the cities that were matched and then filtered out because the state columns were different. But I couldn't find the "manhattan" entry here either. I'm not sure where exactly this particular entry got removed, but it's definitely possible that something similar has happened to other major cities.


## Manual Matching
We can try to bring the major cities that have been dropped back into this merge manually.

Creating a dataframe to store our manual matches:
```{r}
manual_matches <- head(cities, 0)
manual_matches <- cbind(manual_matches, head(census_places, 0))
# colnames(manual_matches) <- c(paste0("cities_", colnames(cities)), paste0("census_places_", colnames(census_places)))
colnames(manual_matches) <- c(paste0(colnames(cities)), paste0(colnames(census_places)))
```

Let's first try matching Manhattan-New York manually:
```{r}
# Manhattan
match_nyc_city <- cities[cities$citypermid == 148, ]
match_nyc_census <- census_places[census_places$PLACE == "new york city", ]
manual_matches <- rbind(manual_matches, c(match_nyc_city, match_nyc_census))
```
This is kind of tedious, we can write a function to speed it up a bit:
```{r}
new_manual_match <- function(manual_matches, id_cities, id_census_places) {
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$PLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- rbind(manual_matches, new_match)
  return(manual_matches)
}
```

Let's test the function:
```{r}
# Test with Bronx-New York match
manual_matches <- new_manual_match(manual_matches, 1944, "new york city")
```

Now we can use a hashmap to store all of our manual matches and just pass that to the function. That way if we want to add a new match we can just add it to the hashmap.

```{r}
manual_matches_map <- list(
  "queens" = list(permid = 2042, place= "new york city"),
  "brooklyn" = list(permid = 394, place= "new york city"),
  "staten island" = list(permid = 1647, place = "new york city"),
  "south chicago heights" = list(permid = 766, place = "south chicago heights village"),
  "austin texas" = list(permid = 549, place = "austin city"),
  "columbus ohio" = list(permid = 386, place = "columbus city"),
  "indianapolis" = list(permid = 319, place = "indianapolis city (remainder)"), # what does remainder mean?
  "nashville" = list(permid = 47037, place = "nashville-davidson metropolitan government (balance)"),
  "las vegas" = list(permid = 1839, place = "las vegas city"),
  "mesa" = list(permid = 20, place = "mesa city"),
  "miami" = list(permid = 62, place = "miami city"), #census data contains both miami and miami beach
  "virginia beach" = list(permid = 215, place = "virginia beach city") #include norfolk as well?
  
)
```


Iterate over this hashmap and add each entry to our manual_matches dataframe.
```{r}
for (i in manual_matches_map) {
  id_cities <- i$permid
  id_census_places <- i$place
  match_cities <- cities[cities$citypermid == id_cities, ]
  match_census <- census_places[census_places$PLACE == id_census_places, ]
  new_match <- c(match_cities, match_census)
  manual_matches <- rbind(manual_matches, new_match)
}
```
Some places can't be identified by their census place name because there are multiple entries for one place name. Let's fix these manually:
```{r}
manual_matches <- rbind(manual_matches, c(cities[cities$citypermid == 533, ], census_places[census_places$NHGISPLACE == "G410590000", ])) #Portland, OR match
manual_matches <- rbind(manual_matches, c(cities[cities$citypermid == 2023, ], census_places[census_places$NHGISPLACE == "G480040000", ]))
```
Now we have a dataframe complete with our manual matches that can easily be added to and appended to our main dataframe.
